[behavior]
; Whether or not to decode UTF-8 in messages & responses
decodeUTF8=false
; You can specify names of entire message groups to scan
msgGroups=
; subCodes is a comma-separted list of internal Synchronet
; sub-board codes to specify which message sub-boards to
; use
subCodes=
; excludeSubCodes is similar to the above, but specifies
; which sub-boards to exclude/ignore
excludeSubCodes=
; The name to use for posting messages
botName=
; Whether or not to look for & respond to messages written to 'All'
lookForMsgsToAll=true
; Whether or not to look for & respond to messages written to
; the bot name
lookForMsgsToBot=true
; A comma-separated list of other names to look for (messages
; posted by) to respond to
otherNamesToLookFor=
; A comma-separated list of 'from' names to ignore in messages
fromNamesToIgnore=
; A comma-separated list of specific subjects to look for in
; messages to respond to (you can leave this blank to not
; check for message subjects)
subjectsToLookFor=
; Specifies a timestamp (in UNIX format) to start scanning
; messages at
msgIncludeTime=-6M
; The base filename (without extension) of the file to use
; for saving progress
savedProgressFilenameBase=msgbase_AI_chat
; You can specify a filename of specific 'conversation end'
; phrases to look for - When these are found, the script
; will then change the topic in its reply. You can leave
; this blank to not use 'conversation end' phrases.
conversationEndPhraseFilename=conversationEndPhrases.txt
; The name of a topics file to use for conversation topics
; when replying to messages. This is mainly used when the
; script changes to a new topic.
; In a topics file, each entry is to have 2 lines to specify
; the message subject and new message text, followed by an
; empty line.
; You can leave this setting empty if you wish.
topicsFilename=topics.txt
; You can use the following setting to specify the name
; of a (text) file to use to look for certain words and
; phrases in messages to reply to. Each line can be a
; single exact word or phrase, and can also have the
; wildcard * at the beginning or end to specify that the
; word can contain that.
dictionaryFilenameForWordsAndPhrasesToLookFor=
; The following setting can be used to toggle whether or
; not to use the 'dictionary' of words & phrases loaded
; with the previous setting to reply to messages with
; any of those words or phrases
useDictionaryAndFromAndSubject=false
; With the following setting, you can specify the name of a
; file that will additional, contain custom prompt text to
; send to the ; AI chat bot that you can use to manage the
; responses that come back from the chat bot. You can leave
; this setting empty if you don't want to use this feature.
additionalTextFilename=msgbase_AI_chat_itself_1_additionalRequestTest.txt
; With the AIBackend setting, you can specify which AI
; chat bot to use: Either google_gemini (Google Gemini) or
; openAI (for ChatGPT)
AIBackend=google_gemini
;AIBackend=openAI

[google_gemini]
; Google Gemini API key here
APIKey=
;modelName=gemini-2.0-flash-lite
modelName=gemini-2.5-pro-exp-03-25
;The temperature is used for sampling during response generation, which occurs when
;topP and topK are applied.
;Temperature controls the degree of randomness in token selection. Lower temperatures are
;good for prompts that require a less open-ended or creative response, while higher
;temperatures can lead to more diverse or creative results. A temperature of 0 means that
;the highest probability tokens are always selected. In this case, responses for a given
;prompt are mostly deterministic, but a small amount of variation is still possible.
;If the model returns a response that's too generic, too short, or the model gives a
;fallback response, try increasing the temperature.
;
;Range for gemini-1.5-flash: 0.0 - 2.0 (default: 1.0)
;Range for gemini-1.5-pro: 0.0 - 2.0 (default: 1.0)
;Range for gemini-1.0-pro-vision: 0.0 - 1.0 (default: 0.4)
;Range for gemini-1.0-pro-002: 0.0 - 2.0 (default: 1.0)
;Range for gemini-1.0-pro-001: 0.0 - 1.0 (default: 0.9)
temperature=2
;topP: If specified, nucleus sampling is used.
;Top-P changes how the model selects tokens for output. Tokens are selected from the most
;(see top-K) to least probable until the sum of their probabilities equals the top-P value.
;For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P
;value is 0.5, then the model will select either A or B as the next token by using temperature
;and excludes C as a candidate.
;Specify a lower value for less random responses and a higher value for more random responses.
;
;Range: 0.0 - 1.0
;Default for gemini-1.5-flash: 0.95
;Default for gemini-1.5-pro: 0.95
;Default for gemini-1.0-pro: 1.0
;Default for gemini-1.0-pro-vision: 1.0
topP=0.95
;Top-K changes how the model selects tokens for output. A top-K of 1 means the next selected
;token is the most probable among all tokens in the model's vocabulary (also called greedy
;decoding), while a top-K of 3 means that the next token is selected from among the three
;most probable tokens by using temperature.
;For each token selection step, the top-K tokens with the highest probabilities are sampled.
;Then tokens are further filtered based on top-P with the final token selected using temperature
;sampling.
;Specify a lower value for less random responses and a higher value for more random responses.
;Range: 1-40
;Supported by gemini-1.0-pro-vision only.
;Default for gemini-1.0-pro-vision: 32
topK=40

[openAI]
; openAI/ChatGPT API key here
APIKey=
modelName=gpt-4o-mini